{
  "paragraphs": [
    {
      "text": "%md\n\n## Accessing MapR-DB Binary tables in Zeppelin\nThis section contains an example of an Apache Spark job that uses the MapR-DB Binary Connector for Apache Spark to write and read a MapR-DB Binary table.",
      "user": "anonymous",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eAccessing MapR-DB Binary tables in Zeppelin\u003c/h2\u003e\n\u003cp\u003eThis section contains an example of an Apache Spark job that uses the MapR-DB Binary Connector for Apache Spark to write and read a MapR-DB Binary table.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517256223648_300104267",
      "id": "20180129-200343_1997599312",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Reading and writing data using DataFrame",
      "text": "%livy\nimport org.apache.hadoop.fs.{FileSystem, Path}\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.datasources.hbase.HBaseTableCatalog\n\nval workingDir \u003d \"/user/\" + sc.sparkUser + \"/zeppelin/samples\"\nval tablePath \u003d workingDir + \"/sample_table_binary_df\"\n\n// Create samples directory in MapR-FS\ndef ensureWorkingDir \u003d {\n  val fs \u003d FileSystem.get(sc.hadoopConfiguration)\n  val workingPath \u003d new Path(workingDir)\n  if(!fs.exists(workingPath)) fs.mkdirs(workingPath)\n}\nensureWorkingDir\n\ncase class HBaseRecord(\n  col0: String,\n  col1: Boolean,\n  col2: Double,\n  col3: Float,\n  col4: Int,\n  col5: Long,\n  col6: Short,\n  col7: String,\n  col8: Byte)\n\nval data \u003d (0 to 255).map { i \u003d\u003e\n  val s \u003d \"row\" + \"%03d\".format(i)\n  new HBaseRecord(s,\n    i % 2 \u003d\u003d 0,\n    i.toDouble,\n    i.toFloat,\n    i,\n    i.toLong,\n    i.toShort,\n    s\"String $i extra\",\n    i.toByte)\n}\n\nval catalog \u003d s\"\"\"{\n    |\"table\": {\"namespace\": \"default\", \"name\": \"$tablePath\"},\n    |\"rowkey\": \"key\",\n    |\"columns\": {\n      |\"col0\": {\"cf\": \"rowkey\", \"col\": \"key\", \"type\": \"string\"},\n      |\"col1\": {\"cf\": \"cf1\", \"col\": \"col1\", \"type\": \"boolean\"},\n      |\"col2\": {\"cf\": \"cf2\", \"col\": \"col2\", \"type\": \"double\"},\n      |\"col3\": {\"cf\": \"cf3\", \"col\": \"col3\", \"type\": \"float\"},\n      |\"col4\": {\"cf\": \"cf4\", \"col\": \"col4\", \"type\": \"int\"},\n      |\"col5\": {\"cf\": \"cf5\", \"col\": \"col5\", \"type\": \"bigint\"},\n      |\"col6\": {\"cf\": \"cf6\", \"col\": \"col6\", \"type\": \"smallint\"},\n      |\"col7\": {\"cf\": \"cf7\", \"col\": \"col7\", \"type\": \"string\"},\n      |\"col8\": {\"cf\": \"cf8\", \"col\": \"col8\", \"type\": \"tinyint\"}\n    |}\n  |}\"\"\".stripMargin\n\n\nval rdd \u003d sc.parallelize(data).toDF.write.options(\n  Map(HBaseTableCatalog.tableCatalog -\u003e catalog, HBaseTableCatalog.newTable -\u003e \"5\")).format(\"org.apache.hadoop.hbase.spark\").save()\n\nval df \u003d {\n  spark\n    .read\n    .options(Map(HBaseTableCatalog.tableCatalog -\u003e catalog))\n    .format(\"org.apache.hadoop.hbase.spark\")\n    .load()\n}\n\ndf.show",
      "user": "anonymous",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.hadoop.fs.{FileSystem, Path}\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.datasources.hbase.HBaseTableCatalog\nworkingDir: String \u003d /user/mapr/zeppelin/samples\ntablePath: String \u003d /user/mapr/zeppelin/samples/sample_table_binary_df\nensureWorkingDir: AnyVal\nres45: AnyVal \u003d ()\ndefined class HBaseRecord\ndata: scala.collection.immutable.IndexedSeq[HBaseRecord] \u003d Vector(HBaseRecord(row000,true,0.0,0.0,0,0,0,String 0 extra,0), HBaseRecord(row001,false,1.0,1.0,1,1,1,String 1 extra,1), HBaseRecord(row002,true,2.0,2.0,2,2,2,String 2 extra,2), HBaseRecord(row003,false,3.0,3.0,3,3,3,String 3 extra,3), HBaseRecord(row004,true,4.0,4.0,4,4,4,String 4 extra,4), HBaseRecord(row005,false,5.0,5.0,5,5,5,String 5 extra,5), HBaseRecord(row006,true,6.0,6.0,6,6,6,String 6 extra,6), HBaseRecord(row007,false,7.0,7.0,7,7,7,String 7 extra,7), HBaseRecord(row008,true,8.0,8.0,8,8,8,String 8 extra,8), HBaseRecord(row009,false,9.0,9.0,9,9,9,String 9 extra,9), HBaseRecord(row010,true,10.0,10.0,10,10,10,String 10 extra,10), HBaseRecord(row011,false,11.0,11.0,11,11,11,String 11 extra,11), HBaseRecord(row012,true,12....catalog: String \u003d\n{\n\"table\": {\"namespace\": \"default\", \"name\": \"/user/mapr/zeppelin/samples/sample_table_binary_df\"},\n\"rowkey\": \"key\",\n\"columns\": {\n\"col0\": {\"cf\": \"rowkey\", \"col\": \"key\", \"type\": \"string\"},\n\"col1\": {\"cf\": \"cf1\", \"col\": \"col1\", \"type\": \"boolean\"},\n\"col2\": {\"cf\": \"cf2\", \"col\": \"col2\", \"type\": \"double\"},\n\"col3\": {\"cf\": \"cf3\", \"col\": \"col3\", \"type\": \"float\"},\n\"col4\": {\"cf\": \"cf4\", \"col\": \"col4\", \"type\": \"int\"},\n\"col5\": {\"cf\": \"cf5\", \"col\": \"col5\", \"type\": \"bigint\"},\n\"col6\": {\"cf\": \"cf6\", \"col\": \"col6\", \"type\": \"smallint\"},\n\"col7\": {\"cf\": \"cf7\", \"col\": \"col7\", \"type\": \"string\"},\n\"col8\": {\"cf\": \"cf8\", \"col\": \"col8\", \"type\": \"tinyint\"}\n}\n}\nrdd: Unit \u003d ()\ndf: org.apache.spark.sql.DataFrame \u003d [col4: int, col7: string ... 7 more fields]\n+----+---------------+-----+----+----+------+----+----+----+\n|col4|           col7| col1|col3|col6|  col0|col8|col2|col5|\n+----+---------------+-----+----+----+------+----+----+----+\n|   0| String 0 extra| true| 0.0|   0|row000|   0| 0.0|   0|\n|   1| String 1 extra|false| 1.0|   1|row001|   1| 1.0|   1|\n|   2| String 2 extra| true| 2.0|   2|row002|   2| 2.0|   2|\n|   3| String 3 extra|false| 3.0|   3|row003|   3| 3.0|   3|\n|   4| String 4 extra| true| 4.0|   4|row004|   4| 4.0|   4|\n|   5| String 5 extra|false| 5.0|   5|row005|   5| 5.0|   5|\n|   6| String 6 extra| true| 6.0|   6|row006|   6| 6.0|   6|\n|   7| String 7 extra|false| 7.0|   7|row007|   7| 7.0|   7|\n|   8| String 8 extra| true| 8.0|   8|row008|   8| 8.0|   8|\n|   9| String 9 extra|false| 9.0|   9|row009|   9| 9.0|   9|\n|  10|String 10 extra| true|10.0|  10|row010|  10|10.0|  10|\n|  11|String 11 extra|false|11.0|  11|row011|  11|11.0|  11|\n|  12|String 12 extra| true|12.0|  12|row012|  12|12.0|  12|\n|  13|String 13 extra|false|13.0|  13|row013|  13|13.0|  13|\n|  14|String 14 extra| true|14.0|  14|row014|  14|14.0|  14|\n|  15|String 15 extra|false|15.0|  15|row015|  15|15.0|  15|\n|  16|String 16 extra| true|16.0|  16|row016|  16|16.0|  16|\n|  17|String 17 extra|false|17.0|  17|row017|  17|17.0|  17|\n|  18|String 18 extra| true|18.0|  18|row018|  18|18.0|  18|\n|  19|String 19 extra|false|19.0|  19|row019|  19|19.0|  19|\n+----+---------------+-----+----+----+------+----+----+----+\nonly showing top 20 rows"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517255317573_1078963044",
      "id": "20180129-194837_1856731574",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Reading and writing data using RDD",
      "text": "%livy\nimport org.apache.hadoop.fs.{FileSystem, Path}\nimport org.apache.hadoop.hbase.client.{HBaseAdmin, Put, Get, Result}\nimport org.apache.hadoop.hbase.spark.HBaseContext\nimport org.apache.hadoop.hbase.spark.HBaseRDDFunctions._\nimport org.apache.hadoop.hbase.util.{Bytes \u003d\u003e By}\nimport org.apache.hadoop.hbase.{CellUtil, HBaseConfiguration, HColumnDescriptor, HTableDescriptor, TableName}\n\nval workingDir \u003d \"/user/\" + sc.sparkUser + \"/zeppelin/samples\"\nval tablePath \u003d workingDir + \"/sample_table_binary_rdd\"\nval columnFamily \u003d \"sample_cf\"\n\n// Create samples directory in MapR-FS\ndef ensureWorkingDir \u003d {\n  val fs \u003d FileSystem.get(sc.hadoopConfiguration)\n  val workingPath \u003d new Path(workingDir)\n  if(!fs.exists(workingPath)) fs.mkdirs(workingPath)\n}\nensureWorkingDir\n\n// Initialize HBaseContext\ndef createHBaseContext \u003d {\n  val hbaseConf \u003d HBaseConfiguration.create()\n  val hbaseContext \u003d new HBaseContext(sc, hbaseConf)\n  hbaseContext\n}\nval hbaseContext \u003d createHBaseContext\n\n// Create empty table\ndef ensureTable \u003d {\n  val hbaseConf \u003d HBaseConfiguration.create()\n  val hbaseAdmin \u003d new HBaseAdmin(hbaseConf)\n  if (hbaseAdmin.tableExists(tablePath)) {\n      hbaseAdmin.disableTable(tablePath)\n      hbaseAdmin.deleteTable(tablePath)\n  }\n  val tableDesc \u003d new HTableDescriptor(tablePath)\n  tableDesc.addFamily(new HColumnDescriptor(By.toBytes(columnFamily)))\n  hbaseAdmin.createTable(tableDesc)\n}\nensureTable\n\n\n// Put data into table\nval putRDD \u003d sc.parallelize(Array(\n  (By.toBytes(\"1\"), (By.toBytes(columnFamily), By.toBytes(\"1\"), By.toBytes(\"1\"))),\n  (By.toBytes(\"2\"), (By.toBytes(columnFamily), By.toBytes(\"1\"), By.toBytes(\"2\"))),\n  (By.toBytes(\"3\"), (By.toBytes(columnFamily), By.toBytes(\"1\"), By.toBytes(\"3\"))),\n  (By.toBytes(\"4\"), (By.toBytes(columnFamily), By.toBytes(\"1\"), By.toBytes(\"4\"))),\n  (By.toBytes(\"5\"), (By.toBytes(columnFamily), By.toBytes(\"1\"), By.toBytes(\"5\")))\n))\n\nputRDD.hbaseBulkPut(hbaseContext, TableName.valueOf(tablePath),\n  (putRecord) \u003d\u003e {\n    val put \u003d new Put(putRecord._1)\n    val (family, qualifier, value) \u003d putRecord._2\n    put.addColumn(family, qualifier, value)\n    put\n  })\n\n// Get data from table\nval getRDD \u003d sc.parallelize(Array(\n  By.toBytes(\"5\"),\n  By.toBytes(\"4\"),\n  By.toBytes(\"3\"),\n  By.toBytes(\"2\"),\n  By.toBytes(\"1\")\n))\n\nval resRDD \u003d getRDD.hbaseBulkGet[String](hbaseContext, TableName.valueOf(tablePath), 2,\n  (record) \u003d\u003e { new Get(record) },\n  (result: Result) \u003d\u003e {\n    val it \u003d result.listCells().iterator()\n    val sb \u003d new StringBuilder\n\n    sb.append(By.toString(result.getRow) + \": \")\n    while (it.hasNext) {\n      val cell \u003d it.next()\n      val q \u003d By.toString(CellUtil.cloneQualifier(cell))\n      if (q.equals(\"counter\")) {\n        sb.append(\"(\" + q + \",\" + By.toLong(CellUtil.cloneValue(cell)) + \")\")\n      } else {\n        sb.append(\"(\" + q + \",\" + By.toString(CellUtil.cloneValue(cell)) + \")\")\n      }\n    }\n    sb.toString()\n  })\n\nresRDD.collect().foreach(v \u003d\u003e println(v))",
      "user": "anonymous",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala",
        "lineNumbers": false,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.hadoop.fs.{FileSystem, Path}\nimport org.apache.hadoop.hbase.client.{HBaseAdmin, Put, Get, Result}\nimport org.apache.hadoop.hbase.spark.HBaseContext\nimport org.apache.hadoop.hbase.spark.HBaseRDDFunctions._\nimport org.apache.hadoop.hbase.util.{Bytes\u003d\u003eBy}\nimport org.apache.hadoop.hbase.{CellUtil, HBaseConfiguration, HColumnDescriptor, HTableDescriptor, TableName}\nworkingDir: String \u003d /user/mapr/zeppelin/samples\ntablePath: String \u003d /user/mapr/zeppelin/samples/sample_table_binary_rdd\ncolumnFamily: String \u003d sample_cf\nensureWorkingDir: AnyVal\nres57: AnyVal \u003d ()\ncreateHBaseContext: org.apache.hadoop.hbase.spark.HBaseContext\nhbaseContext: org.apache.hadoop.hbase.spark.HBaseContext \u003d org.apache.hadoop.hbase.spark.HBaseContext@7eda3374\nwarning: there were two deprecation warnings; re-run with -deprecation for details\nensureTable: Unit\nputRDD: org.apache.spark.rdd.RDD[(Array[Byte], (Array[Byte], Array[Byte], Array[Byte]))] \u003d ParallelCollectionRDD[42] at parallelize at \u003cconsole\u003e:58\ngetRDD: org.apache.spark.rdd.RDD[Array[Byte]] \u003d ParallelCollectionRDD[43] at parallelize at \u003cconsole\u003e:55\nresRDD: org.apache.spark.rdd.RDD[String] \u003d MapPartitionsRDD[44] at mapPartitions at HBaseContext.scala:388\n5: (1,5)\n4: (1,4)\n3: (1,3)\n2: (1,2)\n1: (1,1)"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517566433507_1621939549",
      "id": "20180202-101353_1150659090",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "MapR Tutorial/Spark MapR-DB Binary Connector (Scala)",
  "id": "2D6JT1W6P",
  "angularObjects": {},
  "config": {},
  "info": {}
}
