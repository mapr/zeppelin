{
  "paragraphs": [
    {
      "text": "%md\n\n## Accessing MapR-DB Binary tables in Zeppelin\nThis section contains an example of an Apache Spark job that uses the MapR-DB Binary Connector for Apache Spark to write and read a MapR-DB Binary table.",
      "user": "anonymous",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eAccessing MapR-DB Binary tables in Zeppelin\u003c/h2\u003e\n\u003cp\u003eThis section contains an example of an Apache Spark job that uses the MapR-DB Binary Connector for Apache Spark to write and read a MapR-DB Binary table.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517256223648_300104267",
      "id": "20180129-200343_1997599312",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Reading and writing data using DataFrame",
      "text": "%livy\nimport org.apache.hadoop.fs.{FileSystem, Path}\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.datasources.hbase.HBaseTableCatalog\n\nval workingDir \u003d \"/user/\" + sc.sparkUser + \"/zeppelin/samples\"\nval tablePath \u003d workingDir + \"/sample_table_binary_df\"\n\nval fs \u003d FileSystem.get(sc.hadoopConfiguration)\nval workingPath \u003d new Path(workingDir)\nif(!fs.exists(workingPath)) fs.mkdirs(workingPath)\n\ncase class HBaseRecord(\n  col0: String,\n  col1: Boolean,\n  col2: Double,\n  col3: Float,\n  col4: Int,\n  col5: Long,\n  col6: Short,\n  col7: String,\n  col8: Byte)\n\nval data \u003d (0 to 255).map { i \u003d\u003e\n  val s \u003d \"row\" + \"%03d\".format(i)\n  new HBaseRecord(s,\n      i % 2 \u003d\u003d 0,\n      i.toDouble,\n      i.toFloat,\n      i,\n      i.toLong,\n      i.toShort,\n      s\"String $i extra\",\n      i.toByte)\n}\n\nval catalog \u003d s\"\"\"{\n      |\"table\": {\"namespace\": \"default\", \"name\": \"$tablePath\"},\n      |\"rowkey\": \"key\",\n      |\"columns\": {\n        |\"col0\": {\"cf\": \"rowkey\", \"col\": \"key\", \"type\": \"string\"},\n        |\"col1\": {\"cf\": \"cf1\", \"col\": \"col1\", \"type\": \"boolean\"},\n        |\"col2\": {\"cf\": \"cf2\", \"col\": \"col2\", \"type\": \"double\"},\n        |\"col3\": {\"cf\": \"cf3\", \"col\": \"col3\", \"type\": \"float\"},\n        |\"col4\": {\"cf\": \"cf4\", \"col\": \"col4\", \"type\": \"int\"},\n        |\"col5\": {\"cf\": \"cf5\", \"col\": \"col5\", \"type\": \"bigint\"},\n        |\"col6\": {\"cf\": \"cf6\", \"col\": \"col6\", \"type\": \"smallint\"},\n        |\"col7\": {\"cf\": \"cf7\", \"col\": \"col7\", \"type\": \"string\"},\n        |\"col8\": {\"cf\": \"cf8\", \"col\": \"col8\", \"type\": \"tinyint\"}\n      |}\n    |}\"\"\".stripMargin\n\n\nval rdd \u003d sc.parallelize(data).toDF.write.options(\n  Map(HBaseTableCatalog.tableCatalog -\u003e catalog, HBaseTableCatalog.newTable -\u003e \"5\")).format(\"org.apache.hadoop.hbase.spark\").save()\n\nval df \u003d {\n  spark\n    .read\n    .options(Map(HBaseTableCatalog.tableCatalog -\u003e catalog))\n    .format(\"org.apache.hadoop.hbase.spark\")\n    .load()\n}\n\ndf.show",
      "user": "anonymous",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+----+---------------+-----+----+----+------+----+----+----+\n|col4|           col7| col1|col3|col6|  col0|col8|col2|col5|\n+----+---------------+-----+----+----+------+----+----+----+\n|   0| String 0 extra| true| 0.0|   0|row000|   0| 0.0|   0|\n|   1| String 1 extra|false| 1.0|   1|row001|   1| 1.0|   1|\n|   2| String 2 extra| true| 2.0|   2|row002|   2| 2.0|   2|\n|   3| String 3 extra|false| 3.0|   3|row003|   3| 3.0|   3|\n|   4| String 4 extra| true| 4.0|   4|row004|   4| 4.0|   4|\n|   5| String 5 extra|false| 5.0|   5|row005|   5| 5.0|   5|\n|   6| String 6 extra| true| 6.0|   6|row006|   6| 6.0|   6|\n|   7| String 7 extra|false| 7.0|   7|row007|   7| 7.0|   7|\n|   8| String 8 extra| true| 8.0|   8|row008|   8| 8.0|   8|\n|   9| String 9 extra|false| 9.0|   9|row009|   9| 9.0|   9|\n|  10|String 10 extra| true|10.0|  10|row010|  10|10.0|  10|\n|  11|String 11 extra|false|11.0|  11|row011|  11|11.0|  11|\n|  12|String 12 extra| true|12.0|  12|row012|  12|12.0|  12|\n|  13|String 13 extra|false|13.0|  13|row013|  13|13.0|  13|\n|  14|String 14 extra| true|14.0|  14|row014|  14|14.0|  14|\n|  15|String 15 extra|false|15.0|  15|row015|  15|15.0|  15|\n|  16|String 16 extra| true|16.0|  16|row016|  16|16.0|  16|\n|  17|String 17 extra|false|17.0|  17|row017|  17|17.0|  17|\n|  18|String 18 extra| true|18.0|  18|row018|  18|18.0|  18|\n|  19|String 19 extra|false|19.0|  19|row019|  19|19.0|  19|\n+----+---------------+-----+----+----+------+----+----+----+\nonly showing top 20 rows"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517255317573_1078963044",
      "id": "20180129-194837_1856731574",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Reading and writing data using RDD",
      "text": "%livy\nimport org.apache.hadoop.fs.{FileSystem, Path}\nimport org.apache.hadoop.hbase.client.{HBaseAdmin, Put, Get, Result}\nimport org.apache.hadoop.hbase.spark.HBaseContext\nimport org.apache.hadoop.hbase.spark.HBaseRDDFunctions._\nimport org.apache.hadoop.hbase.util.{Bytes \u003d\u003e By}\nimport org.apache.hadoop.hbase.{CellUtil, HBaseConfiguration, HColumnDescriptor, HTableDescriptor, TableName}\n\n\nval workingDir \u003d \"/user/\" + sc.sparkUser + \"/zeppelin/samples\"\nval tablePath \u003d workingDir + \"/sample_table_binary_rdd\"\nval columnFamily \u003d \"sample_cf\"\n\n\n// Clean directory in MapR-FS\nval fs \u003d FileSystem.get(sc.hadoopConfiguration)\nval workingPath \u003d new Path(workingDir)\nif(!fs.exists(workingPath)) fs.mkdirs(workingPath)\n\n// Initialize HBaseContext and HBaseAdmin\nval hbaseConf \u003d HBaseConfiguration.create()\nval hbaseContext \u003d new HBaseContext(sc, hbaseConf)\nval hbaseAdmin \u003d new HBaseAdmin(hbaseConf)\n\n// Create empty table\nif (hbaseAdmin.tableExists(tablePath)) {\n    hbaseAdmin.disableTable(tablePath)\n    hbaseAdmin.deleteTable(tablePath)\n}\nval tableDesc \u003d new HTableDescriptor(tablePath)\ntableDesc.addFamily(new HColumnDescriptor(By.toBytes(columnFamily)))\nhbaseAdmin.createTable(tableDesc)\n\n\n// Put data into table\nval putRDD \u003d sc.parallelize(Array(\n  (By.toBytes(\"1\"), (By.toBytes(columnFamily), By.toBytes(\"1\"), By.toBytes(\"1\"))),\n  (By.toBytes(\"2\"), (By.toBytes(columnFamily), By.toBytes(\"1\"), By.toBytes(\"2\"))),\n  (By.toBytes(\"3\"), (By.toBytes(columnFamily), By.toBytes(\"1\"), By.toBytes(\"3\"))),\n  (By.toBytes(\"4\"), (By.toBytes(columnFamily), By.toBytes(\"1\"), By.toBytes(\"4\"))),\n  (By.toBytes(\"5\"), (By.toBytes(columnFamily), By.toBytes(\"1\"), By.toBytes(\"5\")))\n))\n\nputRDD.hbaseBulkPut(hbaseContext, TableName.valueOf(tablePath),\n  (putRecord) \u003d\u003e {\n    val put \u003d new Put(putRecord._1)\n    val (family, qualifier, value) \u003d putRecord._2\n    put.addColumn(family, qualifier, value)\n    put\n  })\n\n// Get data from table\nval getRDD \u003d sc.parallelize(Array(\n    By.toBytes(\"5\"),\n    By.toBytes(\"4\"),\n    By.toBytes(\"3\"),\n    By.toBytes(\"2\"),\n    By.toBytes(\"1\")\n))\n\nval resRDD \u003d getRDD.hbaseBulkGet[String](hbaseContext, TableName.valueOf(tablePath), 2,\n    (record) \u003d\u003e { new Get(record) },\n    (result: Result) \u003d\u003e {\n      val it \u003d result.listCells().iterator()\n      val sb \u003d new StringBuilder\n\n      sb.append(By.toString(result.getRow) + \": \")\n      while (it.hasNext) {\n        val cell \u003d it.next()\n        val q \u003d By.toString(CellUtil.cloneQualifier(cell))\n        if (q.equals(\"counter\")) {\n          sb.append(\"(\" + q + \",\" + By.toLong(CellUtil.cloneValue(cell)) + \")\")\n        } else {\n          sb.append(\"(\" + q + \",\" + By.toString(CellUtil.cloneValue(cell)) + \")\")\n        }\n      }\n      sb.toString()\n    })\n\nresRDD.collect().foreach(v \u003d\u003e println(v))",
      "user": "anonymous",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala",
        "lineNumbers": false,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "5: (1,5)\n4: (1,4)\n3: (1,3)\n2: (1,2)\n1: (1,1)"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517566433507_1621939549",
      "id": "20180202-101353_1150659090",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "MapR Tutorial/Spark MapR-DB Binary Connector (Scala)",
  "id": "2D6JT1W6P",
  "angularObjects": {},
  "config": {},
  "info": {}
}
